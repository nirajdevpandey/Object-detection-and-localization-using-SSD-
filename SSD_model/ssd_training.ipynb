{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SSD training utils.\"\"\"\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Multibox loss with some helper functions.\n",
    " ### Arguments\n",
    "        num_classes: Number of classes including background.\n",
    "        alpha: Weight of L1-smooth loss.\n",
    "        neg_pos_ratio: Max ratio of negative to positive boxes in loss.\n",
    "        background_label_id: Id of background label.\n",
    "        negatives_for_hard: Number of negative boxes to consider\n",
    "            it there is no positive boxes in batch.\n",
    " ### References\n",
    "       https://arxiv.org/abs/1512.02325\n",
    "### TODO\n",
    "        Add possibility for background label id be not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiboxLoss(object):\n",
    "    def __init__(self, num_classes, alpha=1.0, neg_pos_ratio=3.0,\n",
    "                 background_label_id=0, negatives_for_hard=100.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        if background_label_id != 0:\n",
    "            raise Exception('Only 0 as background label id is supported')\n",
    "        self.background_label_id = background_label_id\n",
    "        self.negatives_for_hard = negatives_for_hard\n",
    "\n",
    "    def _l1_smooth_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute L1-smooth loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth bounding boxes,\n",
    "                tensor of shape (?, num_boxes, 4).\n",
    "            y_pred: Predicted bounding boxes,\n",
    "                tensor of shape (?, num_boxes, 4).\n",
    "        # Returns\n",
    "            l1_loss: L1-smooth loss, tensor of shape (?, num_boxes).\n",
    "        # References\n",
    "            https://arxiv.org/abs/1504.08083\n",
    "        \"\"\"\n",
    "        abs_loss = tf.abs(y_true - y_pred)\n",
    "        sq_loss = 0.5 * (y_true - y_pred)**2\n",
    "        l1_loss = tf.where(tf.less(abs_loss, 1.0), sq_loss, abs_loss - 0.5)\n",
    "        return tf.reduce_sum(l1_loss, -1)\n",
    "\n",
    "    def _softmax_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute softmax loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth targets,\n",
    "                tensor of shape (?, num_boxes, num_classes).\n",
    "            y_pred: Predicted logits,\n",
    "                tensor of shape (?, num_boxes, num_classes).\n",
    "        # Returns\n",
    "            softmax_loss: Softmax loss, tensor of shape (?, num_boxes).\n",
    "        \"\"\"\n",
    "        y_pred = tf.maximum(tf.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "        softmax_loss = -tf.reduce_sum(y_true * tf.log(y_pred),\n",
    "                                      axis=-1)\n",
    "        return softmax_loss\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute mutlibox loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth targets,\n",
    "                tensor of shape (?, num_boxes, 4 + num_classes + 8),\n",
    "                priors in ground truth are fictitious,\n",
    "                y_true[:, :, -8] has 1 if prior should be penalized\n",
    "                    or in other words is assigned to some ground truth box,\n",
    "                y_true[:, :, -7:] are all 0.\n",
    "            y_pred: Predicted logits,\n",
    "                tensor of shape (?, num_boxes, 4 + num_classes + 8).\n",
    "        # Returns\n",
    "            loss: Loss for prediction, tensor of shape (?,).\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(y_true)[0]\n",
    "        num_boxes = tf.to_float(tf.shape(y_true)[1])\n",
    "\n",
    "        # loss for all priors\n",
    "        conf_loss = self._softmax_loss(y_true[:, :, 4:-8],\n",
    "                                       y_pred[:, :, 4:-8])\n",
    "        loc_loss = self._l1_smooth_loss(y_true[:, :, :4],\n",
    "                                        y_pred[:, :, :4])\n",
    "\n",
    "        # get positives loss\n",
    "        num_pos = tf.reduce_sum(y_true[:, :, -8], axis=-1)\n",
    "        pos_loc_loss = tf.reduce_sum(loc_loss * y_true[:, :, -8],\n",
    "                                     axis=1)\n",
    "        pos_conf_loss = tf.reduce_sum(conf_loss * y_true[:, :, -8],\n",
    "                                      axis=1)\n",
    "\n",
    "        # get negatives loss, we penalize only confidence here\n",
    "        num_neg = tf.minimum(self.neg_pos_ratio * num_pos,\n",
    "                             num_boxes - num_pos)\n",
    "        pos_num_neg_mask = tf.greater(num_neg, 0)\n",
    "        has_min = tf.to_float(tf.reduce_any(pos_num_neg_mask))\n",
    "        num_neg = tf.concat(axis=0, values=[num_neg,\n",
    "                                [(1 - has_min) * self.negatives_for_hard]])\n",
    "        num_neg_batch = tf.reduce_min(tf.boolean_mask(num_neg,\n",
    "                                                      tf.greater(num_neg, 0)))\n",
    "        num_neg_batch = tf.to_int32(num_neg_batch)\n",
    "        confs_start = 4 + self.background_label_id + 1\n",
    "        confs_end = confs_start + self.num_classes - 1\n",
    "        max_confs = tf.reduce_max(y_pred[:, :, confs_start:confs_end],\n",
    "                                  axis=2)\n",
    "        _, indices = tf.nn.top_k(max_confs * (1 - y_true[:, :, -8]),\n",
    "                                 k=num_neg_batch)\n",
    "        batch_idx = tf.expand_dims(tf.range(0, batch_size), 1)\n",
    "        batch_idx = tf.tile(batch_idx, (1, num_neg_batch))\n",
    "        full_indices = (tf.reshape(batch_idx, [-1]) * tf.to_int32(num_boxes) +\n",
    "                        tf.reshape(indices, [-1]))\n",
    "        # full_indices = tf.concat(2, [tf.expand_dims(batch_idx, 2),\n",
    "        #                              tf.expand_dims(indices, 2)])\n",
    "        # neg_conf_loss = tf.gather_nd(conf_loss, full_indices)\n",
    "        neg_conf_loss = tf.gather(tf.reshape(conf_loss, [-1]),\n",
    "                                  full_indices)\n",
    "        neg_conf_loss = tf.reshape(neg_conf_loss,\n",
    "                                   [batch_size, num_neg_batch])\n",
    "        neg_conf_loss = tf.reduce_sum(neg_conf_loss, axis=1)\n",
    "\n",
    "        # loss is sum of positives and negatives\n",
    "        total_loss = pos_conf_loss + neg_conf_loss\n",
    "        total_loss /= (num_pos + tf.to_float(num_neg_batch))\n",
    "        num_pos = tf.where(tf.not_equal(num_pos, 0), num_pos,\n",
    "                            tf.ones_like(num_pos))\n",
    "        total_loss += (self.alpha * pos_loc_loss) / num_pos\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
